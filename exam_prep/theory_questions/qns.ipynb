{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7472b31c",
   "metadata": {},
   "source": [
    "![fun](images/fun7dof.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660e7ed6",
   "metadata": {},
   "source": [
    "![nonmaxsup](images/nonmaxsuppress.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f7838f",
   "metadata": {},
   "source": [
    "![pose](images/staticsceneposeestimation.png) \\\n",
    "Scale Ambiguity in Translation:\n",
    "\n",
    "The essential matrix only gives the direction of translation, not its magnitude. \\\n",
    "The rotation R is fully determined from the essential matrix (assuming no noise or degeneracies). There is no scale ambiguity here because rotations are inherently normalized (orthonormal matrices).\n",
    "\n",
    "Option\t|| Statement\t|| Why It's Incorrect \\\n",
    "a\tCannot find the pose.\tPose can be recovered up to scale. \\\n",
    "b\tRotation up to arbitrary rotation and translation up to scale.\tNo arbitrary rotation ambiguity exists. \\\n",
    "d\tTranslation without ambiguity, but not rotation.\tTranslation has scale ambiguity; rotation is fully recoverable. \\\n",
    "e\tPose without any ambiguity.\tScale ambiguity in translation prevents this. \\\n",
    "f\tRotation but not translation.\tTranslation is recoverable (up to scale). \\\n",
    "g\tTranslation up to scale but not rotation.\tRotation is fully recoverable. \\\n",
    "h\tRotation up to arbitrary rotation, but not translation.\tNo arbitrary rotation ambiguity; translation is recoverable (up to scale)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7329287b",
   "metadata": {},
   "source": [
    "![workflowsift](images/sift_descriptors_workflow.png)\n",
    "\n",
    "Scale-Space Extrema Detection:\n",
    "\n",
    "* Application of a Gaussian filter at various scales to identify potential keypoints (Option 6).\n",
    "\n",
    "This builds a \"scale space\" to find stable features across different scales.\n",
    "\n",
    "* Keypoint Localization:\n",
    "\n",
    "Candidates are refined to eliminate low-contrast points and edge responses.\n",
    "\n",
    "* Orientation Assignment:\n",
    "\n",
    "Assignment of one or more dominant orientations based on local gradients (Option 7).\n",
    "\n",
    "This ensures rotation invariance by aligning the descriptor to the dominant orientation.\n",
    "\n",
    "* Descriptor Generation:\n",
    "\n",
    "Calculation of gradient magnitude and orientation around the keypoint (Option 3).\n",
    "\n",
    "Division of the region into subregions (typically 4x4) to capture spatial information (Option 5).\n",
    "\n",
    "Creation of a histogram of gradient orientations (8 bins per subregion) (Option 4).\n",
    "\n",
    "Aggregation of histograms from all subregions to form the final 128-dimensional descriptor (Option 8).\n",
    "\n",
    "Normalization of the descriptor to enhance invariance to illumination changes (Option 2).\n",
    "\n",
    "Rotation of the descriptor according to the dominant orientation (Option 9)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cc6487",
   "metadata": {},
   "source": [
    "Option || Step\t|| Relevance to SIFT\n",
    "* 2\tNormalization of the descriptor\tEnsures illumination invariance.\n",
    "* 3\tGradient magnitude/orientation\tFundamental to descriptor creation.\n",
    "* 4\tHistogram of gradient orientations\tCore step for capturing local patterns.\n",
    "* 5\tDivision into subregions\tEncodes spatial information.\n",
    "* 6\tGaussian filtering at various scales\tDetects scale-invariant keypoints.\n",
    "* 7\tDominant orientation assignment\tAchieves rotation invariance.\n",
    "* 8\tAggregation of histograms\tForms the final descriptor.\n",
    "* 9\tDescriptor rotation\tAligns to dominant orientation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a4c9f3",
   "metadata": {},
   "source": [
    "![zhang](images/zhang_algo1.png)\\\n",
    "Why Other Options are Incorrect:\n",
    "a) Incorrect. Skew is not always assumed to be zero. Zhang's algorithm can estimate skew if it exists (e.g., for non-rectangular pixels).\\\n",
    "\n",
    "b) Incorrect. The right singular vector of the largest singular value is not directly used. Instead, SVD is used to solve a system of equations derived from homographies.\\\n",
    "\n",
    "c) Incorrect. While a single image can provide constraints, multiple images (typically 5-10) are needed for robust calibration.\\\n",
    "\n",
    "d) Incorrect. The world points lie on a plane (e.g.,Z=0), but this plane does not need to be parallel to the image plane. The algorithm works with arbitrary plane orientations.\\\n",
    "\n",
    "e) Incorrect. The ratio of world points to image points is irrelevant. The algorithm relies on geometric constraints, not point counts.\\\n",
    "\n",
    "f) Incorrect. The principal point is estimated, not assumed to be at the image center.\\\n",
    "\n",
    "g) Incorrect. While a single image can provide constraints, multiple images are required for full calibration. Also, skew and principal point are estimated, not assumed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5384422",
   "metadata": {},
   "source": [
    "![zhang](images/zhang2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40144dc9",
   "metadata": {},
   "source": [
    "✔️ Zhang’s algorithm only works for flat calibration objects (e.g., checkerboards).\n",
    "\n",
    "Why?\n",
    "\n",
    "Zhang’s method assumes the calibration object lies on a plane (\n",
    "Z\n",
    "=\n",
    "0\n",
    "Z=0 in its local coordinates).\n",
    "\n",
    "It estimates homographies between the planar object and the camera, then solves for intrinsics/extrinsics.\n",
    "\n",
    "Key Insight: Non-flat objects (e.g., 3D grids) violate the planar assumption, breaking the algorithm.\n",
    "\n",
    "Why Other Options Are Wrong:\n",
    "\n",
    "○ \"Small field of view\": False—Zhang’s works for wide FoV if corners are detectable.\n",
    "\n",
    "○ \"Adjusts focus\": Irrelevant; calibration doesn’t involve lens adjustments.\n",
    "\n",
    "○ \"Decomposing \n",
    "P\n",
    "P\": Incorrect—intrinsics come from homography constraints, not \n",
    "P\n",
    "P.\n",
    "\n",
    "○ \"Corners directly minimize error\": Partly true, but corners are used to estimate homographies first.\n",
    "\n",
    "○ \"Homographies constrain fundamental matrix\": False—homographies constrain intrinsics, not \n",
    "F\n",
    "F.\n",
    "\n",
    "○ \"Requires distance/size\": False—only the checkerboard’s pattern size (e.g., square width) is needed, not its distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a00b13",
   "metadata": {},
   "source": [
    "![image](images/EFdiff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f5fc99",
   "metadata": {},
   "source": [
    "Option\t|| Statement\t|| Why It’s Incorrect\n",
    "* 2\tEssential matrix = two images; fundamental matrix = single image.\tBoth relate two images. Neither operates within a single image.\n",
    "* 3\tFundamental matrix = extrinsic; essential matrix = intrinsic.\tF depends on both intrinsic/extrinsic implicitly; E requires intrinsics explicitly.\n",
    "* 4\tFundamental matrix = pinhole; essential matrix = complex lenses.\tBoth assume pinhole models. Lens systems are irrelevant here.\n",
    "* 5\tFundamental = perspective; essential = orthographic.\tBoth assume perspective projection. Orthographic models use different math.\n",
    "* 6\tEssential matrix = subset of F with zero translation.\tE is derived from F with known intrinsics, but translation need not be zero.\n",
    "* 7\tEssential = linear; fundamental = non-linear.\tBoth can be estimated linearly (e.g., 8-point algorithm). Non-linear refinement is optional for both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f077bba",
   "metadata": {},
   "source": [
    "![e_pose](images/e_pose.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87364057",
   "metadata": {},
   "source": [
    "![images](images/levenberg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7305f591",
   "metadata": {},
   "source": [
    "![33](images/33.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed814f6d",
   "metadata": {},
   "source": [
    "![scalar](images/scalar.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af52c7c",
   "metadata": {},
   "source": [
    "![sift2](images/sift2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f64daa4",
   "metadata": {},
   "source": [
    "![coidim](images/codim_ransac.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34433fa0",
   "metadata": {},
   "source": [
    "![stitch](images/stiching.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97493849",
   "metadata": {},
   "source": [
    "![image](images/whattoransac.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cddf65",
   "metadata": {},
   "source": [
    "Option\t|| Why It’s Incorrect\n",
    "Homography (H)\tAssumes the scene is planar (invalid for general 3D odometry). \\\n",
    "Fundamental Matrix (F)\tRequires more points (7–8) and doesn’t directly give \n",
    "R, t. Also for uncalibrated cameras. \\\n",
    "Essential Matrix (DoF argument)\tWrong reasoning.  \\\n",
    "E has 5 DoF (not \"more\"), but this isn’t why it’s better. \\\n",
    "Direct solvePnP\tRequires known 3D points (not available in monocular odometry’s first frame). \\\n",
    "Fundamental Matrix (DoF argument)\tF has 7 DoF, but this isn’t relevant for pose estimation. \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4f6284",
   "metadata": {},
   "source": [
    "![image](images/K.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c38c455",
   "metadata": {},
   "source": [
    "![image](images/sine.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
